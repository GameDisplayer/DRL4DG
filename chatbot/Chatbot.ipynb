{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "built-playlist",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "generic-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-sculpture",
   "metadata": {},
   "source": [
    "## Load & Preprocess Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-disability",
   "metadata": {},
   "source": [
    "Let's have a look to our dialydialog dataset : https://www.aclweb.org/anthology/I17-1099/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "amateur-picking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kitchen stinks . __eou__ I'll throw out the garbage . __eou__\n",
      "\n",
      "So Dick , how about getting some coffee for tonight ? __eou__ Coffee ? I don ’ t honestly like that kind of stuff . __eou__ Come on , you can at least try a little , besides your cigarette . __eou__ What ’ s wrong with that ? Cigarette is the thing I go crazy for . __eou__ Not for me , Dick . __eou__\n",
      "\n",
      "Are things still going badly with your houseguest ? __eou__ Getting worse . Now he ’ s eating me out of house and home . I ’ Ve tried talking to him but it all goes in one ear and out the other . He makes himself at home , which is fine . But what really gets me is that yesterday he walked into the living room in the raw and I had company over ! That was the last straw . __eou__ Leo , I really think you ’ re beating around the bush with this guy . I know he used to be your best friend in college , but I really think it ’ s time to lay down the law . __eou__ You ’ re right . Everything is probably going to come to a head tonight . I ’ ll keep you informed . __eou__\n",
      "\n",
      "Would you mind waiting a while ? __eou__ Well , how long will it be ? __eou__ I'm not sure . But I'll get a table ready as fast as I can . __eou__ OK . We'll wait . __eou__\n",
      "\n",
      "Are you going to the annual party ? I can give you a ride if you need one . __eou__ Thanks a lot . That's the favor I was going to ask you for . __eou__ The pleasure is mine . __eou__\n",
      "\n",
      "Isn ’ t he the best instructor ? I think he ’ s so hot . Wow ! I really feel energized , don ’ t you ? __eou__ I swear , I ’ m going to kill you for this . __eou__ What ’ s wrong ? Didn ’ t you think it was fun ? ! __eou__ Oh , yeah ! I had a blast ! I love sweating like a pig with a bunch of pot bellies who all smell bad . Sorry , I ’ m just not into this health kick . __eou__ Oh , no , get off it . It wasn ’ t such a killer class . You just have to get into it . Like they say , no pain , no gain . __eou__ I am wiped out . Thank you . __eou__ Look , next time get yourself some comfy shoes . You ’ re gonna come back again with me , aren ’ t you ? __eou__ Never ! But thank you for inviting me . __eou__ Come on . You ’ ll feel better after we hit the showers . __eou__\n",
      "\n",
      "Can I take your order now or do you still want to look at the menu ? __eou__ Well , I want a fillet steak , medium , but my little girl doesn't care for steak . Could she have something else instead ? __eou__ Certainly . How about spaghetti with clams and shrimps . __eou__ Sounds delicious . OK . She'll try that . __eou__\n",
      "\n",
      "Can you manage chopsticks ? __eou__ Why not ? See . __eou__ Good mastery . How do you like our Chinese food ? __eou__ Oh , great ! It's delicious . You see , I am already putting on weight . There is one thing I don't like however , MSG . __eou__ What's wrong with MSG ? It helps to bring out the taste of the food . __eou__ According to some studies it may cause cancer . __eou__ Oh , don't let that worry you . If that were true , China wouldn't have such a large population . __eou__ I just happen to have a question for you guys . Why do the Chinese cook the vegetables ? You see what I mean is that most vitamin are destroyed when heated . __eou__ I don't know exactly . It's a tradition . Maybe it's for sanitary reasons . __eou__\n",
      "\n",
      "I'm exhausted . __eou__ Okay , let's go home . __eou__\n",
      "\n",
      "Good evening . Welcome to Cherry's . Do you have a reservation ? __eou__ No , we don't . __eou__ How many of you , please ? __eou__ Six , including two kids . __eou__ I'm afraid all the big tables are taken . __eou__\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_name = \"dailydialog\"\n",
    "corpus = os.path.join(\"data\", corpus_name)\n",
    "\n",
    "def printLines(file, n=10):\n",
    "    with open(file, 'r', encoding=\"utf-8\") as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)\n",
    "\n",
    "printLines(os.path.join(corpus, \"dialogues_text.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-adapter",
   "metadata": {},
   "source": [
    "## Create formatted data file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-warren",
   "metadata": {},
   "source": [
    "We'll create a formatted data file in which each line contains a tab-separated query sentence and a response sentence pair.\n",
    "\n",
    "The following functions pase the `dailogues_text.txt` data file.\n",
    "- `loadLines` splits each line of the file into conversations\n",
    "- `extractSentencePairs` extracts pairs of sentences from conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acknowledged-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits each line of the file into a dictionary of fields\n",
    "def loadLines(fileName):\n",
    "    conversations = []\n",
    "    with open(fileName, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split(\"__eou__\")\n",
    "            conversations.append(values)\n",
    "    return conversations\n",
    "\n",
    "\n",
    "# Extracts pairs of sentences from conversations\n",
    "def extractSentencePairs(conversations):\n",
    "    qa_pairs = []\n",
    "    for conversation in conversations:\n",
    "        # Iterate over all the lines of the conversation\n",
    "        for i in range(len(conversation) - 1):  # We ignore the last line (no answer for it)\n",
    "            inputLine = conversation[i].strip()\n",
    "            targetLine = conversation[i+1].strip()\n",
    "            # Filter wrong samples (if one of the lists is empty)\n",
    "            if inputLine and targetLine:\n",
    "                qa_pairs.append([inputLine, targetLine])\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-grove",
   "metadata": {},
   "source": [
    "Now we call the above functions to create a new file : `formatted_dialogues_text.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "removable-circular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading conversations...\n",
      "\n",
      "Writing newly formatted file...\n",
      "\n",
      "Sample lines from file:\n",
      "The kitchen stinks .\tI'll throw out the garbage .\n",
      "\n",
      "So Dick , how about getting some coffee for tonight ?\tCoffee ? I don ’ t honestly like that kind of stuff .\n",
      "\n",
      "Coffee ? I don ’ t honestly like that kind of stuff .\tCome on , you can at least try a little , besides your cigarette .\n",
      "\n",
      "Come on , you can at least try a little , besides your cigarette .\tWhat ’ s wrong with that ? Cigarette is the thing I go crazy for .\n",
      "\n",
      "What ’ s wrong with that ? Cigarette is the thing I go crazy for .\tNot for me , Dick .\n",
      "\n",
      "Are things still going badly with your houseguest ?\tGetting worse . Now he ’ s eating me out of house and home . I ’ Ve tried talking to him but it all goes in one ear and out the other . He makes himself at home , which is fine . But what really gets me is that yesterday he walked into the living room in the raw and I had company over ! That was the last straw .\n",
      "\n",
      "Getting worse . Now he ’ s eating me out of house and home . I ’ Ve tried talking to him but it all goes in one ear and out the other . He makes himself at home , which is fine . But what really gets me is that yesterday he walked into the living room in the raw and I had company over ! That was the last straw .\tLeo , I really think you ’ re beating around the bush with this guy . I know he used to be your best friend in college , but I really think it ’ s time to lay down the law .\n",
      "\n",
      "Leo , I really think you ’ re beating around the bush with this guy . I know he used to be your best friend in college , but I really think it ’ s time to lay down the law .\tYou ’ re right . Everything is probably going to come to a head tonight . I ’ ll keep you informed .\n",
      "\n",
      "Would you mind waiting a while ?\tWell , how long will it be ?\n",
      "\n",
      "Well , how long will it be ?\tI'm not sure . But I'll get a table ready as fast as I can .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define path to new file\n",
    "datafile = os.path.join(corpus, \"formatted_dialogues_text.txt\")\n",
    "\n",
    "delimiter = '\\t'\n",
    "# Unescape the delimiter\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "\n",
    "print(\"\\nLoading conversations...\")\n",
    "conversations = loadLines(os.path.join(corpus, \"dialogues_text.txt\"))\n",
    "\n",
    "# Write new csv file\n",
    "print(\"\\nWriting newly formatted file...\")\n",
    "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
    "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
    "    for pair in extractSentencePairs(conversations):\n",
    "        writer.writerow(pair)\n",
    "\n",
    "# Print a sample of lines\n",
    "print(\"\\nSample lines from file:\")\n",
    "printLines(datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-ethernet",
   "metadata": {},
   "source": [
    "## Load and trim data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-circle",
   "metadata": {},
   "source": [
    "Now let's crzate a vocabulary and load query/response sentence pairs into memory.\n",
    "\n",
    "First we must create a mapping of each word to a discrete numerical space (the index value).\n",
    "\n",
    "Voc class keeps the mapping from words to indexes, a reverse mapping of indexes to words, a count of each word and a total word count.\n",
    "There are 3 central methods :\n",
    "- `addWord` to add a word to the vocabulary\n",
    "- `addSentence` to add all words in a sentence\n",
    "- `trim` for trimming infrequently seen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "conventional-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-timing",
   "metadata": {},
   "source": [
    "Before assemble our vocabulary and query/response sentence pairs we must perform some preprocessing.\n",
    "\n",
    "1. Convert the Unicode strings to ASCII with `unicodeToAscii`.\n",
    "2. Convert all letters to lowercase and trim all non-letter characters except basic punctuation `normalizeString`\n",
    "3. Filter out sentences witg length greater than the `MAX_LENGTH` threshold in `filterPairs`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "grateful-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 15  # Maximum sentence length to consider\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Read query/response pairs and return a voc object\n",
    "def readVocs(datafile, corpus_name):\n",
    "    print(\"Reading lines...\")\n",
    "    # Read the file and split into lines\n",
    "    lines = open(datafile, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs\n",
    "\n",
    "# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "# Filter pairs using filterPair condition\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-survivor",
   "metadata": {},
   "source": [
    "Finally assmble voc and pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "middle-dealing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 89862 sentence pairs\n",
      "Trimmed to 43724 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 10119\n",
      "\n",
      "pairs:\n",
      "['the kitchen stinks .', 'i ll throw out the garbage .']\n",
      "['so dick how about getting some coffee for tonight ?', 'coffee ? i don t honestly like that kind of stuff .']\n",
      "['coffee ? i don t honestly like that kind of stuff .', 'come on you can at least try a little besides your cigarette .']\n",
      "['would you mind waiting a while ?', 'well how long will it be ?']\n",
      "['i swear i m going to kill you for this .', 'what s wrong ? didn t you think it was fun ? !']\n",
      "['never ! but thank you for inviting me .', 'come on . you ll feel better after we hit the showers .']\n",
      "['certainly . how about spaghetti with clams and shrimps .', 'sounds delicious . ok . she ll try that .']\n",
      "['can you manage chopsticks ?', 'why not ? see .']\n",
      "['why not ? see .', 'good mastery . how do you like our chinese food ?']\n",
      "['i m exhausted .', 'okay let s go home .']\n"
     ]
    }
   ],
   "source": [
    "# Load/Assemble voc and pairs\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-spread",
   "metadata": {},
   "source": [
    "### Trimming rarely used words out of vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-artwork",
   "metadata": {},
   "source": [
    "One tactic beneficial to achieve faster convergence during training is trimming rarely used words out of our vocabulary.\n",
    "\n",
    "1. Trim words used under `MIN_COUNT` threshold using `voc.trim`\n",
    "2. Filter out pairs with trimmed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "specific-munich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 6157 / 10116 = 0.6086\n",
      "Trimmed from 43724 pairs to 38716, 0.8855 of total\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# Trim voc and pairs\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-vertical",
   "metadata": {},
   "source": [
    "## Prepare Data for Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-basement",
   "metadata": {},
   "source": [
    "BATCH TECHNIQUE\n",
    "\n",
    "To accomodate sentences of different sizes in the same batch we make our batched input tensor of shape `(max_length, batch_size)` where sentences shorter than the max_length are zeropadded after the `EOS_token`.\n",
    "\n",
    "- `inputVar` function handles the process of converting sentences to tensor. It returns a tensor of `lengths` for each sequence in the batch for the decoder.\n",
    "- `outputVar` function performs the same as `inputVar` but returns a binary mask tensor and a maximum target sentence length.\n",
    "- `batch2TrainData` takes a bunch of pairs and returns the input and target tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "recovered-nowhere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[   6,    6,   31,  154,  125],\n",
      "        [ 166,   51,  274,   48,    6],\n",
      "        [  31, 1163, 1417,  159,   32],\n",
      "        [ 114,  101,   53,  721,    5],\n",
      "        [1272,   31,   96,   20,    2],\n",
      "        [   9,   20,    5,    2,    0],\n",
      "        [  18,    2,    2,    0,    0],\n",
      "        [  55,    0,    0,    0,    0],\n",
      "        [3417,    0,    0,    0,    0],\n",
      "        [   5,    0,    0,    0,    0],\n",
      "        [   2,    0,    0,    0,    0]])\n",
      "lengths: tensor([11,  7,  7,  6,  5])\n",
      "target_variable: tensor([[  66,  103,   25,  182,   48],\n",
      "        [  31,    6,   57,    5,  154],\n",
      "        [   5,  580,  189,    6,  721],\n",
      "        [   2,   22,    5,   24,  440],\n",
      "        [   0,    5,    2,    3,   22],\n",
      "        [   0,   86,    0, 5340,   48],\n",
      "        [   0,   20,    0,   57,   20],\n",
      "        [   0,    2,    0,  513,    2],\n",
      "        [   0,    0,    0,  201,    0],\n",
      "        [   0,    0,    0,    5,    0],\n",
      "        [   0,    0,    0,    2,    0]])\n",
      "mask: tensor([[ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [False,  True,  True,  True,  True],\n",
      "        [False,  True, False,  True,  True],\n",
      "        [False,  True, False,  True,  True],\n",
      "        [False,  True, False,  True,  True],\n",
      "        [False, False, False,  True, False],\n",
      "        [False, False, False,  True, False],\n",
      "        [False, False, False,  True, False]])\n",
      "max_target_len: 11\n"
     ]
    }
   ],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.BoolTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-happiness",
   "metadata": {},
   "source": [
    "## Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-float",
   "metadata": {},
   "source": [
    "The brain of our chatbot is a sequence to sequence model.\n",
    "\n",
    "One RNN acts as an _encoder_  which encodes a variable length input sequence to a fixed-length context vector (the final hidden layer of the RNN).\n",
    "The second RNN is a _decoder_ which takes a s input a word and a context vector and returns a guess for the next word in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-quantum",
   "metadata": {},
   "source": [
    " ### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-might",
   "metadata": {},
   "source": [
    "The encoder RNN iterates through tokens and outputs an \"output\" vector and a \"hidden sate\" vector. the hidden state vector is passed to the next time step while the output vector is recorder.\n",
    "The encoder transforms the context it saw at each point in the sequence into a set of points in a high dimensional space. The decoder will use it to generate the outputted word.\n",
    "\n",
    "We use a bidirectional multi-layered Gated Recurrent Unit.\n",
    "It gives the advantage of encoding both past and future context !\n",
    "\n",
    "Computation Graph :\n",
    "1. Convert word indexed to embeddings\n",
    "2. Pack padded batch of sequences for RNN module\n",
    "3. Forward pass through GRU\n",
    "4. Unpack padding\n",
    "5. Sum bidirectional GRU outputs\n",
    "6. Return output and final hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "conventional-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-jewelry",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-ecuador",
   "metadata": {},
   "source": [
    "The decoder RNN uses the encoder's context vectors and internal hidden states to generate the next word of the sequence.\n",
    "It continues generating words until an `EOS_token`.\n",
    "The problem with a vanilla seq2seq decoder is that if we rely woley on the context vector it will have information loss. (especially with long input sequences).\n",
    "\n",
    "-> `attention mechanism` allows the decoder to pay attention to certain parts of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "varying-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-starter",
   "metadata": {},
   "source": [
    "Now that the attention submodule is implemented let's dive into the actual decoder model.\n",
    "\n",
    "Computation Graph:\n",
    "1. Get embedding of current input word.\n",
    "2. Forward through unidirectional GRU\n",
    "3. Calculate attention weights from the current GRU output\n",
    "4. Multiply attention weights to encoder outputs to get a new context vector\n",
    "5. Concatenate weighted context vector and GRU using Luong eq\n",
    "6. Predict next word\n",
    "7. Return output and final hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "supported-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-drilling",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-joshua",
   "metadata": {},
   "source": [
    "`maskNNLLLoss` calculates the average negative log likelihood of the elements that correspond to a 1 in the mask tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "divided-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-apparel",
   "metadata": {},
   "source": [
    "Single training iteration (single batch of inputs)\n",
    "\n",
    "Couple of clever tricks :\n",
    "- `teacher forcing` at some probability (set by `teacher_forcing_ratio`) current target word is used as the decoder's next input rather than using the decoder's current guess.\n",
    "- `gradient clipping` commonly used technique for countering the \"exploding gradient\" problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-crime",
   "metadata": {},
   "source": [
    "Sequence of operations :\n",
    "1. Forward pass entire input batch through encoder\n",
    "2. initialize decoder inputs as SOS_token and hidden state as the encoder's final hidden state\n",
    "3. Forward input batch sequence through decoder one time step at a time\n",
    "4. If teacher forcing : set next decoder input as the current target else : set next decoder input as the current decoder ouptput\n",
    "5. Calculate and accumulate loss\n",
    "6. Perform backpropagation\n",
    "7. Clip gradients\n",
    "8. Update encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rough-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    # Lengths for rnn packing should always be on the cpu\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-bloom",
   "metadata": {},
   "source": [
    "Training iterations + save our model to run inferences or continue training !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "surface-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-branch",
   "metadata": {},
   "source": [
    "## Gready decoding for generating sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-extent",
   "metadata": {},
   "source": [
    "Decoding method when training is not using teacher forcing. At each time step we choose the word from decoder_output with the highest softmax value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "accompanied-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-yellow",
   "metadata": {},
   "source": [
    "## Evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "square-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-maple",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-admission",
   "metadata": {},
   "source": [
    "possible to laod from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "guided-implement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Configure models\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "#loadFilename = None\n",
    "checkpoint_iter = 5000 # 4000\n",
    "loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "#print(loadFilename)\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    # If loading on same machine the model was trained on\n",
    "    #checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-somalia",
   "metadata": {},
   "source": [
    "## Train !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "variable-enemy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 5000 #4000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# If you have cuda, configure cuda to call\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-washington",
   "metadata": {},
   "source": [
    "## RUN AN PLAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "anticipated-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting\n",
    "#evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-necessity",
   "metadata": {},
   "source": [
    "## Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-automation",
   "metadata": {},
   "source": [
    "### Forward and backward models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "funded-xerox",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "forward_decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "forward_encoder = forward_encoder.to(device)\n",
    "forward_decoder = forward_decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "informal-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "backward_decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "backward_encoder = backward_encoder.to(device)\n",
    "backward_decoder = backward_decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-consolidation",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "false-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertResponse(response):\n",
    "    size1 = len(response)\n",
    "    size2 = batch_size\n",
    "    npRes = np.zeros((size1, size2), dtype=np.int64)\n",
    "    npLengths = np.zeros(size2, dtype=np.int64)\n",
    "    for i in range(size1):\n",
    "        prov = response[i].cpu().numpy()\n",
    "        for j in range(prov.size):\n",
    "            npLengths[j] = npLengths[j] + 1\n",
    "            if prov.size > 1:\n",
    "                npRes[i][j] = prov[j]\n",
    "            else:\n",
    "                npRes[i][j] = prov \n",
    "    res = torch.from_numpy(npRes)\n",
    "    lengths = torch.from_numpy(npLengths)\n",
    "    return res, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ranking-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertTarget(target):\n",
    "    size1 = len(target)\n",
    "    size2 = batch_size\n",
    "    npRes = np.zeros((size1, size2), dtype=np.int64)\n",
    "    mask = np.zeros((size1, size2), dtype=np.bool_)\n",
    "    npLengths = np.zeros(size2, dtype=np.int64)\n",
    "    for i in range(size1):\n",
    "        prov = target[i].numpy()\n",
    "        for j in range(prov.size):\n",
    "            npLengths[j] = npLengths[j] + 1\n",
    "            if prov.size > 1:\n",
    "                npRes[i][j] = prov[j]\n",
    "            else:\n",
    "                npRes[i][j] = prov \n",
    "                \n",
    "            if npRes[i][j] > 0:\n",
    "                mask[i][j] = True\n",
    "            else:\n",
    "                mask[i][j] = False\n",
    "            \n",
    "    res = torch.from_numpy(npRes)\n",
    "    lengths = torch.from_numpy(npLengths)\n",
    "    max_target_len = torch.max(lengths)\n",
    "    return res, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "australian-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformTensorToSameShapeAs(tensor, shape):\n",
    "    size1, size2 = shape\n",
    "    npNewT = np.zeros((size1, size2), dtype=np.int64)\n",
    "    npNewMask = np.zeros((size1, size2), dtype=np.bool_)\n",
    "    tensorSize1, tensorSize2 = tensor.size()\n",
    "    for i in range(tensorSize1):\n",
    "        for j in range(tensorSize2):\n",
    "            npNewT[i][j] = tensor[i][j]\n",
    "            npNewMask[i][j]= True\n",
    "    return torch.from_numpy(npNewT), torch.from_numpy(npNewMask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-north",
   "metadata": {},
   "source": [
    "### Training step for a single iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "technological-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RL(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, batch_size, teacher_forcing_ratio):\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    # Lengths for rnn packing should always be on the cpu\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "    \n",
    "    #Initialize variables\n",
    "    loss=0\n",
    "    #print_losses = []\n",
    "    response=[]\n",
    "    \n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "    \n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    \n",
    "    \n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "     # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "     # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            #print_losses.append(mask_loss.item() * nTotal)\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            #print_losses.append(mask_loss.item() * nTotal)\n",
    "            \n",
    "            #ni or decoder_output\n",
    "            response.append(topi)\n",
    "            \n",
    "    return loss, max_target_len, response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-allergy",
   "metadata": {},
   "source": [
    "Let's define the rewards method for each action.\n",
    "\n",
    "It is composed of 3 different types of rewards :\n",
    "- `ease of answering` which is basically saying that a turn generated by a machineshould be easy to respond to. \n",
    "- `information flow` : We  want  the  agent  to  contribute new information at each turn to keep the dialogue moving and avoid repetitive sequences. \n",
    "- `semantic coherence` measures the adequacy of responses to avoid situations in which the generated replies are highly rewarded but are ungrammatical or not coherent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "elegant-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def easeOfAnswering(input_variable, lengths, dull_responses, mask, max_target_len, encoder, decoder, batch_size, teacher_forcing_ratio):\n",
    "    NS=len(dull_responses)\n",
    "    r1=0\n",
    "    for d in dull_responses:\n",
    "        voc.addSentence(d)\n",
    "        d, mask, max_target_len = outputVar(d, voc)\n",
    "        newD, newMask = transformTensorToSameShapeAs(d, input_variable.size())\n",
    "        #tar, mask, max_target_len = convertTarget(d)\n",
    "        forward_loss, forward_len, _ = RL(input_variable, lengths, newD, newMask, max_target_len, encoder, decoder, batch_size, teacher_forcing_ratio)\n",
    "        # log (1/P(a|s)) = CE  --> log(P(a | s)) = - CE\n",
    "        if forward_len > 0:\n",
    "            r1 -= forward_loss / forward_len\n",
    "    if len(dull_responses) > 0:\n",
    "        r1 = r1 / NS\n",
    "    return r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "indie-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "def informationFlow(responses):\n",
    "    r2=0\n",
    "    if(len(responses) > 2):\n",
    "        #2 representations obtained from the encoder for two consecutive turns pi and pi+1\n",
    "        h_pi = responses[-3]\n",
    "        h_pi1 = responses[-1]\n",
    "        # length of the two vector might not match\n",
    "        min_length = min(len(h_pi), len(h_pi+1))\n",
    "        h_pi = h_pi[:min_length]\n",
    "        h_pi1 = h_pi1[:min_length]\n",
    "        #cosine similarity \n",
    "        #cos_sim = 1 - distance.cosine(h_pi, h_pi1)\n",
    "        cos_sim = 1 - distance.cdist(h_pi.cpu().numpy(), h_pi1.cpu().numpy(), 'cosine')\n",
    "        #Handle negative cos_sim\n",
    "        if np.any(cos_sim <= 0):\n",
    "            r2 = - cos_sim\n",
    "        else:\n",
    "            r2 = - np.log(cos_sim)\n",
    "        r2 = np.mean(r2)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "raised-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semanticCoherence(input_variable, lengths, target_variable, mask, max_target_len, forward_encoder, forward_decoder, backward_encoder, backward_decoder, batch_size, teacher_forcing_ratio):\n",
    "    r3 = 0\n",
    "    forward_loss, forward_len, _ = RL(input_variable, lengths, target_variable, mask, max_target_len, forward_encoder, forward_decoder, batch_size, teacher_forcing_ratio)\n",
    "    backward_loss, backward_len, _ = RL(target_variable, lengths, input_variable, mask, max_target_len, backward_encoder, backward_decoder, batch_size, teacher_forcing_ratio)\n",
    "    if forward_len > 0:\n",
    "        r3 += forward_loss / forward_len\n",
    "    if backward_len > 0:\n",
    "        r3+= backward_loss / backward_len\n",
    "    return r3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "thrown-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=0.25\n",
    "l2=0.25\n",
    "l3=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "brown-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rewards(input_var, lengths, target_var, mask, max_target_len, forward_encoder, forward_decoder, backward_encoder, backward_decoder, batch_size, teacher_forcing_ratio):\n",
    "    #rewards per episode\n",
    "    ep_rewards = []\n",
    "    #indice of current episode\n",
    "    ep_num = 1\n",
    "    #list of responses\n",
    "    responses = []\n",
    "    #input of current episode\n",
    "    ep_input = input_var\n",
    "    #target of current episode\n",
    "    ep_target = target_var\n",
    "    \n",
    "    #ep_num bounded -> to redefine (MEDIUM POST)\n",
    "    while (ep_num <= 10):\n",
    "        \n",
    "        print(ep_num)\n",
    "        #generate current response with the forward model\n",
    "        _, _, curr_response = RL(ep_input, lengths, ep_target, mask, max_target_len, forward_encoder, forward_decoder, batch_size, teacher_forcing_ratio)\n",
    "        \n",
    "        #Break if :\n",
    "        # 1 -> dull response\n",
    "        # 2 -> response is less than MIN_LENGTH\n",
    "        # 3 -> repetition ie curr_response in responses\n",
    "        if(len(curr_response) < MIN_COUNT):# or (curr_response in dull_responses) or (curr_response in responses)):\n",
    "            break\n",
    "            \n",
    "            \n",
    "        #We can add the response to responses list\n",
    "        #curr_response = torch.LongTensor(curr_response).view(-1, 1)\n",
    "        #transform curr_response size\n",
    "        #target = torch.zeros(960, 1)\n",
    "        #target[:15, :] = curr_response\n",
    "        #curr_response = target\n",
    "        #print(curr_response.size())\n",
    "        #curr_response = torch.reshape(curr_response, (15, 64))\n",
    "        #print(curr_response.size())\n",
    "        #curr_response = curr_response.to(device)\n",
    "        #responses.append(curr_response) \n",
    "        \n",
    "        \n",
    "        #Ease of answering\n",
    "        r1 = easeOfAnswering(ep_input, lengths, dull_responses, mask, max_target_len, forward_encoder, forward_decoder, batch_size, teacher_forcing_ratio)\n",
    "        \n",
    "        #Information flow\n",
    "        r2 = informationFlow(responses)\n",
    "        \n",
    "        #Semantic coherence\n",
    "        r3 = semanticCoherence(ep_input, lengths, target_var, mask, max_target_len, forward_encoder, forward_decoder, backward_encoder, backward_decoder, batch_size, teacher_forcing_ratio)\n",
    "        \n",
    "        #Final reward as a weighted sum of rewards\n",
    "        r = l1*r1 + l2*r2 + l3*r3\n",
    "        \n",
    "        #Add the current reward to the list\n",
    "        ep_rewards.append(r.detach().cpu().numpy())\n",
    "        \n",
    "        #We can add the response to responses list\n",
    "        curr_response, lengths = convertResponse(curr_response)\n",
    "        curr_response = curr_response.to(device)\n",
    "        responses.append(curr_response)\n",
    "        \n",
    "        #Next input is the current response\n",
    "        ep_input = curr_response\n",
    "        #Next target -> dummy\n",
    "        ep_target = torch.zeros(MAX_LENGTH,batch_size,dtype=torch.int64)\n",
    "        #ep_target = torch.LongTensor(torch.LongTensor([0] * MAX_LENGTH)).view(-1, 1)\n",
    "        ep_target = ep_target.to(device)\n",
    "        \n",
    "        #Turn off the teacher forcing  after first iteration -> dummy target\n",
    "        teacher_forcing_ratio = 0\n",
    "        ep_num +=1\n",
    "        \n",
    "    #Take the mean of the episodic rewards\n",
    "    return np.mean(ep_rewards) if len(ep_rewards) > 0 else 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-matter",
   "metadata": {},
   "source": [
    "Dull responses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "actual-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "dull_responses = [\"I do not know what you are talking about.\", \"I don not know.\", \n",
    " \"You do not know.\", \"You know what I mean.\", \"I know what you mean.\", \n",
    " \"You know what I am saying.\", \"You do not know anything.\"]\n",
    "#dull_responses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-blond",
   "metadata": {},
   "source": [
    "### Training RL loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "consolidated-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingRLLoop(model_name, voc, pairs, batch_size, forward_encoder, forward_encoder_optimizer, forward_decoder, forward_decoder_optimizer, backward_encoder, backward_encoder_optimizer, backward_decoder, backward_decoder_optimizer,teacher_forcing_ratio, dull_responses, n_iteration, print_every):\n",
    "    \n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "    \n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    \n",
    "    \n",
    "    #Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        ##MODIFS HERE\n",
    "        # Zero gradients the optimizer\n",
    "        forward_encoder_optimizer.zero_grad()\n",
    "        forward_decoder_optimizer.zero_grad()\n",
    "        \n",
    "        backward_encoder_optimizer.zero_grad()\n",
    "        backward_decoder_optimizer.zero_grad()\n",
    "        \n",
    "        #Forward\n",
    "        forward_loss, forward_len, _ = RL(input_variable, lengths, target_variable, mask, max_target_len, forward_encoder, forward_decoder, batch_size, teacher_forcing_ratio)\n",
    "        \n",
    "        #Calculate reward\n",
    "        reward = calculate_rewards(input_variable, lengths, target_variable, mask, max_target_len, forward_encoder, forward_decoder, backward_encoder, backward_decoder, batch_size, teacher_forcing_ratio)\n",
    "        \n",
    "        #Update forward seq2seq with loss scaled by reward\n",
    "        loss = forward_loss * reward\n",
    "        \n",
    "        loss.backward()\n",
    "        forward_encoder_optimizer.step()\n",
    "        forward_decoder_optimizer.step()\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        print_loss += loss / forward_len\n",
    "        \n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "            \n",
    "        #SAVE CHECKPOINT TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-estimate",
   "metadata": {},
   "source": [
    "## RUN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "southwest-hayes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at C:/cb/pytorch_1000000000000/work/aten/src\\THC/THCReduceAll.cuh:327",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-3d2710504e48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m# Run training iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting Training!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mtrainingRLLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_encoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_encoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_decoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_decoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackward_encoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackward_encoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackward_decoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackward_decoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdull_responses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-fea1329383af>\u001b[0m in \u001b[0;36mtrainingRLLoop\u001b[1;34m(model_name, voc, pairs, batch_size, forward_encoder, forward_encoder_optimizer, forward_decoder, forward_decoder_optimizer, backward_encoder, backward_encoder_optimizer, backward_decoder, backward_decoder_optimizer, teacher_forcing_ratio, dull_responses, n_iteration, print_every)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m#Calculate reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_rewards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_variable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_target_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_encoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_decoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackward_encoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackward_decoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m#Update forward seq2seq with loss scaled by reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-616bd113155b>\u001b[0m in \u001b[0;36mcalculate_rewards\u001b[1;34m(input_var, lengths, target_var, mask, max_target_len, forward_encoder, forward_decoder, backward_encoder, backward_decoder, batch_size, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m#Ease of answering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mr1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0measeOfAnswering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdull_responses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_target_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_encoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_decoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m#Information flow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-f7070c3c80ab>\u001b[0m in \u001b[0;36measeOfAnswering\u001b[1;34m(input_variable, lengths, dull_responses, mask, max_target_len, encoder, decoder, batch_size, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mnewD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewMask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformTensorToSameShapeAs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m#tar, mask, max_target_len = convertTarget(d)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mforward_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_variable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewMask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_target_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;31m# log (1/P(a|s)) = CE  --> log(P(a | s)) = - CE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforward_len\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-00dfa53c1768>\u001b[0m in \u001b[0;36mRL\u001b[1;34m(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, batch_size, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;31m# Calculate and accumulate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mmask_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnTotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaskNLLLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmask_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;31m#print_losses.append(mask_loss.item() * nTotal)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-f319a975feb5>\u001b[0m in \u001b[0;36mmaskNLLLoss\u001b[1;34m(inp, target, mask)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mnTotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcrossEntropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrossEntropy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnTotal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at C:/cb/pytorch_1000000000000/work/aten/src\\THC/THCReduceAll.cuh:327"
     ]
    }
   ],
   "source": [
    "#Configure RL model\n",
    "\n",
    "model_name='RL_model'\n",
    "n_iteration = 4000\n",
    "print_every=100\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "forward_encoder.train()\n",
    "forward_decoder.train()\n",
    "\n",
    "backward_encoder.train()\n",
    "backward_decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "forward_encoder_optimizer = optim.Adam(forward_encoder.parameters(), lr=learning_rate)\n",
    "forward_decoder_optimizer = optim.Adam(forward_decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "backward_encoder_optimizer = optim.Adam(backward_encoder.parameters(), lr=learning_rate)\n",
    "backward_decoder_optimizer = optim.Adam(backward_decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "# If you have cuda, configure cuda to call\n",
    "for state in forward_encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in forward_decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "            \n",
    "for state in backward_encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in backward_decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "            \n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "trainingRLLoop(model_name, voc, pairs, batch_size, forward_encoder, forward_encoder_optimizer, forward_decoder, forward_decoder_optimizer, backward_encoder, backward_encoder_optimizer, backward_decoder, backward_decoder_optimizer, teacher_forcing_ratio, dull_responses, n_iteration, print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dropout layers to eval mode\n",
    "forward_encoder.eval()\n",
    "forward_decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(forward_encoder, forward_decoder)\n",
    "\n",
    "# Begin chatting\n",
    "evaluateInput(forward_encoder, forward_decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-logistics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlbot]",
   "language": "python",
   "name": "conda-env-rlbot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
